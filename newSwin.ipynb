{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1888e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tasni\\miniconda3\\envs\\torch-env\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tasni\\miniconda3\\envs\\torch-env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.7 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.6/10.7 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.7 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e429e929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tasni\\miniconda3\\envs\\torch-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\tasni\\miniconda3\\envs\\torch-env\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from networks.vision_transformer import SwinUnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800d1c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2886e2ff810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    root_path = \"./Dataset\"  # <-- This must exist!\n",
    "    img_size = 64\n",
    "    num_classes = 3\n",
    "    base_lr = 0.01\n",
    "    batch_size = 4\n",
    "    max_epochs = 5\n",
    "    n_gpu = 1\n",
    "    num_workers = 4\n",
    "    eval_interval = 5\n",
    "    seed = 42\n",
    "    snapshot_path = \"./swin_output\"\n",
    "    pretrained_ckpt = \"./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\"\n",
    "\n",
    "args = Config()\n",
    "\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(args.snapshot_path, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ee322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create HepaticDataset\n",
    "from PIL import Image   \n",
    "\n",
    "class HepaticDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return {\n",
    "            'image': image.float(),\n",
    "            'label': mask.long().squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315b871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8006f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transforms and loader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((args.img_size, args.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Get file list\n",
    "image_paths = sorted(os.listdir(os.path.join(args.root_path, '2D_Sliced_Images')))\n",
    "train_files, val_files = train_test_split(image_paths, test_size=0.2, random_state=args.seed)\n",
    "\n",
    "# Update HepaticDataset to take file_list\n",
    "class HepaticDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, file_list, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        mask = Image.open(mask_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return {'image': image.float(), 'label': mask.long().squeeze()}\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((args.img_size, args.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define datasets and loaders\n",
    "train_dataset = HepaticDataset(\n",
    "    os.path.join(args.root_path, '2D_Sliced_Images'),\n",
    "    os.path.join(args.root_path, '2D_Sliced_Masks'),\n",
    "    train_files,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = HepaticDataset(\n",
    "    os.path.join(args.root_path, '2D_Sliced_Images'),\n",
    "    os.path.join(args.root_path, '2D_Sliced_Masks'),\n",
    "    val_files,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3f76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define diceloss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, input, target, smooth=1e-5, softmax=True):\n",
    "        if softmax:\n",
    "            input = torch.softmax(input, dim=1)\n",
    "\n",
    "        target_onehot = torch.eye(self.num_classes)[target].permute(0, 3, 1, 2).to(input.device)\n",
    "\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(input * target_onehot, dims)\n",
    "        cardinality = torch.sum(input + target_onehot, dims)\n",
    "\n",
    "        dice = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "        return 1. - dice.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294243db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads a YAML configuration file and converts it into a CfgNode\n",
    "from yacs.config import CfgNode as CN\n",
    "import yaml\n",
    "\n",
    "# Load YAML into a dictionary\n",
    "with open('configs/swin_tiny_patch4_window7_224_lite.yaml', 'r') as f:\n",
    "    yaml_cfg = yaml.safe_load(f)\n",
    "\n",
    "# Convert dictionary to CfgNode (nested access support)\n",
    "config = CN(yaml_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9743b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "PyTorch CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1627989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tasni\\miniconda3\\envs\\torch-env\\lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---final upsample expand_first---\n",
      "pretrained_path:./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SwinUnet\\networks\\vision_transformer.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(pretrained_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---start load pretrained modle of swin encoder---\n"
     ]
    }
   ],
   "source": [
    "#Load SwinUnet model\n",
    "from yacs.config import CfgNode as CN\n",
    "import torch\n",
    "import os\n",
    "from networks.vision_transformer import SwinUnet\n",
    "\n",
    "# ---- Load YAML config ----\n",
    "from yaml import safe_load\n",
    "\n",
    "with open('configs/swin_tiny_patch4_window7_224_lite.yaml', 'r') as f:\n",
    "    yaml_cfg = safe_load(f)\n",
    "\n",
    "config = CN(yaml_cfg)\n",
    "\n",
    "# ---- Set extra training args ----\n",
    "class Args:\n",
    "    root_path = \"./Dataset\"\n",
    "    img_size = config.DATA.IMG_SIZE\n",
    "    num_classes = 3\n",
    "    base_lr = 0.01\n",
    "    batch_size = 4\n",
    "    max_epochs = 5\n",
    "    n_gpu = 1\n",
    "    num_workers = 4\n",
    "    eval_interval = 5\n",
    "    seed = 42\n",
    "    snapshot_path = \"./swin_output\"\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# ---- Ensure output folder exists ----\n",
    "os.makedirs(args.snapshot_path, exist_ok=True)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# ---- Step 1: Create the model ----\n",
    "model = SwinUnet(\n",
    "    config=config,\n",
    "    img_size=args.img_size,\n",
    "    num_classes=args.num_classes,\n",
    "    zero_head=True\n",
    ")\n",
    "\n",
    "# ---- Step 2: Load pretrained weights from config path ----\n",
    "model.load_from(config)\n",
    "\n",
    "# ---- Step 3: Wrap in DataParallel if multiple GPUs ----\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# ---- Step 4: Move to GPU ----\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c42858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPtimizer and loss functions\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "dice_loss = DiceLoss(num_classes=args.num_classes)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.base_lr, momentum=0.9, weight_decay=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Training Setup ---\n",
    "max_iterations = args.max_epochs * len(train_loader)\n",
    "best_loss = float(\"inf\")\n",
    "iter_num = 0\n",
    "\n",
    "# --- Early Stopping Parameters ---\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "\n",
    "# --- Metric tracking lists ---\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "epoch_dice_scores = []\n",
    "epoch_ious = []\n",
    "\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_dice_scores = []\n",
    "val_ious = []\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "for epoch in range(args.max_epochs):\n",
    "    model.train()\n",
    "    epoch_ce = 0\n",
    "    epoch_dice = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_iou = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\"):\n",
    "        images = batch['image'].cuda()\n",
    "        labels = batch['label'].cuda()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss_ce = ce_loss(outputs, labels)\n",
    "        loss_dice = dice_loss(outputs, labels, softmax=True)\n",
    "        loss = 0.4 * loss_ce + 0.6 * loss_dice\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        lr_ = args.base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_\n",
    "\n",
    "        epoch_ce += loss_ce.item()\n",
    "        epoch_dice += loss_dice.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct = (preds == labels).float().mean().item()\n",
    "        epoch_accuracy += correct\n",
    "        epoch_iou += compute_iou(preds, labels, args.num_classes)\n",
    "\n",
    "    # Averages for the epoch\n",
    "    epoch_ce /= len(train_loader)\n",
    "    epoch_dice /= len(train_loader)\n",
    "    epoch_accuracy /= len(train_loader)\n",
    "    epoch_iou /= len(train_loader)\n",
    "    total_loss = 0.4 * epoch_ce + 0.6 * epoch_dice\n",
    "\n",
    "    epoch_losses.append(total_loss)\n",
    "    epoch_accuracies.append(epoch_accuracy)\n",
    "    epoch_dice_scores.append(1 - epoch_dice)  # Dice Score = 1 - Dice Loss\n",
    "    epoch_ious.append(epoch_iou)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch} took {(end_time - start_time)/60:.2f} minutes\")\n",
    "    print(f\" → Train Loss: {total_loss:.4f} | CE: {epoch_ce:.4f} | Dice Loss: {epoch_dice:.4f}\")\n",
    "    print(f\" → Train Accuracy: {epoch_accuracy:.4f} | Train IoU: {epoch_iou:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_iou = 0\n",
    "    val_dice = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss_ce_val = ce_loss(outputs, labels)\n",
    "            loss_dice_val = dice_loss(outputs, labels)\n",
    "            loss_val = 0.4 * loss_ce_val + 0.6 * loss_dice_val\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == labels).float().mean().item()\n",
    "            val_iou += compute_iou(preds, labels, args.num_classes)\n",
    "            val_dice += 1 - loss_dice_val.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / len(val_loader)\n",
    "    val_iou /= len(val_loader)\n",
    "    val_dice /= len(val_loader)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_ious.append(val_iou)\n",
    "    val_dice_scores.append(val_dice)\n",
    "\n",
    "    print(f\" → Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f} | Val IoU: {val_iou:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Logic ---\n",
    "    if total_loss < best_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(args.snapshot_path, 'best_model.pth'))\n",
    "        best_loss = total_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(args.snapshot_path, 'last_model.pth'))\n",
    "        trigger_times += 1\n",
    "        print(f\"EarlyStopping counter: {trigger_times} out of {patience}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "# === Plotting Accuracy and Loss ===\n",
    "epochs = range(1, len(epoch_losses) + 1)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, epoch_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracies, label='Val Accuracy', marker='x')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, epoch_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Val Loss', marker='x')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
